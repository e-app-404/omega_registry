import sys
from pathlib import Path
import json
sys.path.insert(0, str(Path(__file__).parent.parent / "registry"))
from registry.utils.registry import load_json, load_yaml
from registry.utils.constants import SEMANTIC_ROLE_MAP, ENTITY_ID_SUFFIXES, COMMON_AREAS, FEATURE_ROLES
from registry.utils.pathing import resolve_path, project_root
import os
import collections
import re
import difflib

def resolve_area_id(cluster, area_registry, device_registry, area_config):
    # Try direct room/area assignment from cluster
    area = cluster.get('room')
    if area and area in area_config['area_ids']:
        return area, 'cluster_room'
    # Try alias match (cluster room or cluster_id prefix)
    cluster_room = cluster.get('room', '').lower() if cluster.get('room') else ''
    cluster_prefix = cluster.get('cluster_id', '').split('_')[0].lower()
    for area_id, aliases in area_config['area_aliases'].items():
        if cluster_room in aliases or cluster_prefix in aliases:
            return area_id, 'alias_match'
    # Try device-level fallback (if any entity in cluster has a device with area)
    for entity_id in cluster.get('post_reboot_entity_ids', []):
        entity = area_config['entity_map'].get(entity_id)
        if entity:
            device_id = entity.get('device_id')
            if device_id:
                device = area_config['device_map'].get(device_id)
                if device and device.get('area_id') in area_config['area_ids']:
                    return device.get('area_id'), 'device_fallback'
    return None, 'unresolved'

def canonical_entity_key(eid):
    if not eid:
        return None
    eid = eid.lower()
    # Remove _alpha_, _omega_, _matter_, _tplink_, etc.
    eid = re.sub(r'(_alpha_|_omega_|_matter_|_tplink_).*', '', eid)
    # Remove common suffixes
    eid = re.sub(r'(_occupancy|_motion|_illumination|_sensitivity|_timeout|_connection|_battery|_cloud_connection|_signal_level|_signal_strength)$', '', eid)
    eid = re.sub(r'_+$', '', eid)
    eid = re.sub(r'_\d+$', '', eid)
    return eid

def levenshtein(a, b):
    # Simple Levenshtein distance implementation
    if a == b:
        return 0
    if len(a) < len(b):
        return levenshtein(b, a)
    if len(b) == 0:
        return len(a)
    previous_row = range(len(b) + 1)
    for i, c1 in enumerate(a):
        current_row = [i + 1]
        for j, c2 in enumerate(b):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    return previous_row[-1]

VARIANT_MAP = {
    "occupance": "occupancy",
    "presense": "presence",
    # Add more as needed
}

def try_canonical_key_match(key, diag_map):
    trace = {"canonical_key": key, "attempts": [], "result": None}
    if key is None:
        trace["result"] = {"match_type": "no_match", "reason": "key is None"}
        return None, trace
    # 1. Direct match
    if key in diag_map:
        trace["attempts"].append({"type": "direct", "match": key})
        trace["result"] = {"match_type": "direct", "match": key}
        return diag_map[key], trace
    # 2. Prefix token matching
    tokens = key.split("_")
    for i in range(len(tokens), 1, -1):
        prefix = "_".join(tokens[:i])
        if prefix in diag_map:
            trace["attempts"].append({"type": "prefix_match", "match": prefix})
            trace["result"] = {"match_type": "prefix_match", "match": prefix}
            return diag_map[prefix], trace
    # 3. Variant substitution
    for variant, canonical in VARIANT_MAP.items():
        if variant in key:
            variant_key = key.replace(variant, canonical)
            if variant_key in diag_map:
                trace["attempts"].append({"type": "variant_map", "match": variant_key, "variant": variant, "canonical": canonical})
                trace["result"] = {"match_type": "variant_map", "match": variant_key, "variant": variant, "canonical": canonical}
                return diag_map[variant_key], trace
    # 4. Fuzzy match (Levenshtein â‰¤2)
    for candidate in diag_map.keys():
        dist = levenshtein(key, candidate)
        if dist <= 2:
            trace["attempts"].append({"type": "fuzzy_match", "match": candidate, "edit_distance": dist})
            trace["result"] = {"match_type": "fuzzy_match", "match": candidate, "edit_distance": dist}
            return diag_map[candidate], trace
    trace["result"] = {"match_type": "no_match"}
    return None, trace

def aggressive_canonical_key(entity_id):
    if not entity_id or not isinstance(entity_id, str):
        return None
    # Remove domain prefix
    if "." in entity_id:
        entity_id = entity_id.split(".", 1)[1]
    # Remove known suffixes and substrings
    patterns = [
        r"(_alpha_|_omega_|_beta_|_gamma_|_contact|_sensor|_monitor|_cloud_connection|_motion|_timeout|_battery|_signal_level|_signal_strength|_occupancy|_illumination|_presence|_door|_window|_restart|_sensitivity|_attribute|_matter|_tplink|_tp_link|_wifi|_aeotec|_multipurpose|_sonoff|_tapo|_smartthings|_mtr|_relinked|_enriched|_devtools|_canonical|_patched|_snapshot|_v\\d+|_stage\\d+|_run\\d+|_full|_final|_debug|_log|_trace|_csv|_json|_txt|_tmp|_archive|_output|_input|_data|_role|_area|_zone|_room|_entity|_device|_id|_name|_type|_class|_unique|_core|_main|_base|_test|_sample|_manual|_auto|_fallback|_unknown|_unmatched|_ambiguous|_gray|_true|_false|_none|_null|_other)+$",
        r"(_[a-z]+){1,2}$"
    ]
    key = entity_id.lower()
    for pat in patterns:
        key = re.sub(pat, "", key)
    key = re.sub(r"__+", "_", key)  # collapse double underscores
    key = key.strip("_")
    return key if key else None

def main():
    # Load config
    settings_path = os.path.join(os.path.dirname(__file__), '../settings.conf.yaml')
    settings_yaml = load_yaml(settings_path)
    # PATCH: Use root config as settings
    settings = settings_yaml
    # PATCH: Use 'paths' instead of 'input_paths' and 'output_paths'
    input_paths = settings["paths"]
    output_paths = settings["paths"]
    # --- [AUDIT deprecated path usage] Patch: replaced hardcoded paths with config-driven paths ---
    with open(os.path.join(os.path.dirname(__file__), '../conversation_full_history.log'), "a") as log:
        log.write("[PATCH-CONFIG-CONSISTENCY-V1] [AUDIT deprecated path usage] emit_alpha_sensor_registry.py: replaced hardcoded paths with settings['input_paths']/['output_paths'] from settings.conf.yaml\n")
    area_config = {
        'area_ids': set(),
        'area_aliases': {},
        'entity_map': {},
        'device_map': {}
    }
    # Helper to resolve config paths robustly
    PROJECT_ROOT = Path(__file__).parent.parent.resolve()
    # Patch: Use robust path resolution for all registry files
    input_paths["core_area_registry"] = str(resolve_path(input_paths.get("core_area_registry", "input/core.area_registry")))
    input_paths["core_device_registry"] = str(resolve_path(input_paths.get("core_device_registry", "input/core.device_registry")))
    input_paths["core_entity_registry"] = str(resolve_path(input_paths.get("core_entity_registry", "input/core.entity_registry")))
    # Build area_ids and alias map
    for room in settings.get('rooms', []):
        area_config['area_ids'].add(room['id'])
        area_config['area_aliases'][room['id']] = set([room['id'].lower()] + [a.lower() for a in room.get('aliases', [])])
    # Load area registry for additional aliases
    area_registry = load_json(input_paths["core_area_registry"])
    for area in area_registry['data']['areas']:
        area_config['area_ids'].add(area['id'])
        if area['id'] not in area_config['area_aliases']:
            area_config['area_aliases'][area['id']] = set()
        area_config['area_aliases'][area['id']].add(area['id'].lower())
        if area.get('aliases'):
            for alias in area['aliases']:
                area_config['area_aliases'][area['id']].add(alias.lower())
    # Load device and entity registries
    device_registry = load_json(input_paths["core_device_registry"])
    for device in device_registry['data']['devices']:
        area_config['device_map'][device['id']] = device
    entity_registry = load_json(input_paths["core_entity_registry"])
    for entity in entity_registry['data']['entities']:
        area_config['entity_map'][entity['entity_id']] = entity
    # Load fingerprint map for role/semantic_role/area/tier
    fingerprint_map_path = output_paths.get("entity_fingerprint_map", "output/fingerprinting_run/entity_fingerprint_map.json")
    fingerprint_map_path = str(resolve_path(fingerprint_map_path))
    fingerprint_map = load_json(fingerprint_map_path)
    # PATCH: Merge area_resolution_diagnostics.json into fingerprint_map for area enrichment
    diagnostics_path = str(resolve_path("output/fingerprinting_run/area_resolution_diagnostics.json"))
    merge_failures = []  # <-- Initialize here to fix UnboundLocalError
    if os.path.exists(diagnostics_path):
        diagnostics = load_json(diagnostics_path)
        # PATCH-DIAGNOSTICS-AREA-KEY-CANONICALIZATION-V1: Use diagnostics['canonical_key'] if present
        diag_map = {}
        for d in diagnostics:
            key = d.get("canonical_key")
            if key:
                diag_map[key] = d
            elif "entity_id" in d:
                diag_map[canonical_entity_key(d["entity_id"])] = d
        for pre_id, info in fingerprint_map.items():
            post_id = info.get("post_entity_id")
            orig_id = info.get("entity_id", pre_id)  # fallback to pre_id if missing
            canon_pre_id = aggressive_canonical_key(orig_id)
            # Use aggressive canonical key for diagnostics lookup
            diag = diag_map.get(canon_pre_id)
            if diag:
                info["final_area"] = diag.get("final_area", info.get("area"))
                info["area_inference_source"] = diag.get("area_inference_source", info.get("area_inference_source", "unknown"))
                # PATCH: Also set info["area"] for downstream compatibility
                info["area"] = info["final_area"]
            else:
                merge_failures.append({
                    "post_entity_id": post_id,
                    "entity_id": orig_id,
                    "norm_post_entity_id": canonical_entity_key(post_id),
                    "norm_entity_id": canon_pre_id
                })
        # Emit merge failures log
        debug_dir = os.path.join(os.path.dirname(__file__), '../output/debug')
        os.makedirs(debug_dir, exist_ok=True)
        merge_failures_path = os.path.join(debug_dir, 'diagnostic_merge_failures.log')
        with open(merge_failures_path, 'w') as f:
            for entry in merge_failures:
                f.write(json.dumps(entry) + '\n')
    # Feature roles
    feature_roles = set(FEATURE_ROLES)
    # Build entity_id â†’ fingerprint info, now tracking area_inference_source
    post_entity_info = {}
    for pre_id, info in fingerprint_map.items():
        post_id = info.get("post_entity_id")
        if post_id:
            # PATCH-AREA-CLUSTER-INTEGRATION-V1: Track final_area and area_inference_source
            final_area = info.get("area")
            area_inference_source = info.get("area_inference_source", "unknown")
            # If not present, infer from fallback logic (legacy)
            if not area_inference_source:
                if final_area and final_area != "unknown_area":
                    area_inference_source = "fingerprint"
                elif final_area == "unknown_area":
                    area_inference_source = "unresolvable"
                else:
                    area_inference_source = "unresolvable"
            info["final_area"] = final_area
            info["area_inference_source"] = area_inference_source
            post_entity_info[post_id] = info
    # Load clusters
    clusters_path = output_paths.get("fingerprint_entity_clusters", "output/fingerprint_entity_clusters.v1.json")
    clusters_path = str(resolve_path(clusters_path))
    clusters = load_json(clusters_path)
    # PATCH-AREA-CLUSTER-INTEGRATION-V1: Cluster on final_area, exclude unresolvable
    cluster_groups = collections.defaultdict(lambda: {"post_reboot_entity_ids": [], "features": [], "source_clusters": [], "match_methods": [], "confidence_scores": [], "entity_ids": [], "area_inference_sources": []})
    missing_fingerprint = set()
    fallback_cluster_count = 0
    fallback_cluster_entity_count = 0
    for c in clusters:
        for eid in c.get("post_reboot_entity_ids", []):
            info = post_entity_info.get(eid, {})
            final_area = info.get("final_area", info.get("area"))
            area_inference_source = info.get("area_inference_source", "unknown")
            fingerprint_role = info.get("role")
            semantic_role = info.get("semantic_role") or (fingerprint_role if fingerprint_role else None)
            tier = info.get("tier", "alpha")
            # Exclude if area_inference_source == 'unresolvable'
            if area_inference_source == "unresolvable":
                missing_fingerprint.add(eid)
                continue
            key = (final_area, fingerprint_role, semantic_role, tier)
            post_ids = [eid]
            # PATCH: Track area_inference_source for each entity in cluster
            cluster_groups[key]["area_inference_sources"].append(area_inference_source)
            # PATCH: Count fallback-derived clusters/entities
            if area_inference_source != "fingerprint":
                fallback_cluster_entity_count += 1
            # ...existing code for grouping...
            if semantic_role in feature_roles:
                cluster_groups[key]["features"].append(eid)
            else:
                cluster_groups[key]["post_reboot_entity_ids"].append(eid)
            cluster_groups[key]["source_clusters"].append(c["cluster_id"])
            cluster_groups[key]["match_methods"].extend(c.get("match_methods", []))
            cluster_groups[key]["confidence_scores"].append(info.get("confidence_score", 1.0))
            cluster_groups[key]["entity_ids"].append(eid)
    # PATCH: Count clusters with any fallback-derived area
    for (area, role, semantic_role, tier), group in cluster_groups.items():
        if any(src != "fingerprint" for src in group["area_inference_sources"]):
            fallback_cluster_count += 1
    # Build output clusters
    alpha_groups = []
    debug_log = {"clusters": [], "missing_fingerprint": list(missing_fingerprint)}
    seen_entity_ids = set()
    seen_incomplete_entity_ids = set()
    total_incomplete_clusters = 0
    for (area, role, semantic_role, tier), group in cluster_groups.items():
        cluster_id = f"{area}_{role}_{semantic_role}_{tier}"
        post_ids = group["post_reboot_entity_ids"]
        features = group["features"]
        area_inference_sources = group["area_inference_sources"]
        incomplete = False
        incomplete_reasons = list(group.get("incomplete_reasons", []))
        if incomplete_reasons or not area or not role:
            incomplete = True
        if not group["match_methods"]:
            print(f"WARNING: cluster {cluster_id} has no match_methods, using fallback from upstream clusters")
        for eid in post_ids:
            if eid in seen_entity_ids:
                print(f"WARNING: Duplicate entity_id {eid} in multiple clusters")
            if incomplete:
                seen_incomplete_entity_ids.add(eid)
            else:
                seen_entity_ids.add(eid)
        if not post_ids:
            continue
        if incomplete and any(eid in seen_entity_ids for eid in post_ids):
            continue
        if not incomplete and any(eid in seen_incomplete_entity_ids for eid in post_ids):
            continue
        # PATCH: Annotate each entity with area_inference_source
        cluster_obj = {
            "id": cluster_id,
            "area": area,
            "role": role,
            "semantic_role": semantic_role,
            "tier": tier,
            "confidence_score_mean": sum(group["confidence_scores"]) / max(1, len(group["confidence_scores"])),
            "post_reboot_entity_ids": post_ids,
            "features": features,
            "source_clusters": group["source_clusters"],
            "match_methods": group["match_methods"],
            "incomplete": incomplete,
            "area_inference_sources": area_inference_sources
        }
        if incomplete:
            cluster_obj["incomplete_reasons"] = incomplete_reasons
            total_incomplete_clusters += 1
        alpha_groups.append(cluster_obj)
        debug_log["clusters"].append({
            "id": cluster_id,
            "area": area,
            "role": role,
            "semantic_role": semantic_role,
            "tier": tier,
            "post_reboot_entity_ids": post_ids,
            "features": features,
            "incomplete": incomplete,
            "incomplete_reasons": incomplete_reasons,
            "match_methods": group["match_methods"],
            "area_inference_sources": area_inference_sources
        })
    # PATCH: Output fallback cluster/entity counts to debug_log
    debug_log["fallback_cluster_count"] = [int(fallback_cluster_count)]
    debug_log["fallback_cluster_entity_count"] = [int(fallback_cluster_entity_count)]
    # Save outputs
    # Patch: Ensure output directory exists for alpha_sensor_registry.json
    alpha_sensor_registry_path = output_paths.get("alpha_sensor_registry", "output/alpha_sensor_registry.json")
    alpha_sensor_registry_path = str(resolve_path(alpha_sensor_registry_path))
    os.makedirs(os.path.dirname(alpha_sensor_registry_path), exist_ok=True)
    with open(alpha_sensor_registry_path, 'w') as f:
        json.dump(alpha_groups, f, indent=2)
    # Patch: Ensure output directory exists for debug log
    alpha_sensor_registry_debug_path = output_paths.get("alpha_sensor_registry_debug", "output/data/alpha_sensor_registry.debug.log.json")
    alpha_sensor_registry_debug_path = str(resolve_path(alpha_sensor_registry_debug_path))
    os.makedirs(os.path.dirname(alpha_sensor_registry_debug_path), exist_ok=True)
    with open(alpha_sensor_registry_debug_path, 'w') as f:
        json.dump(debug_log, f, indent=2)
    print(f"Alpha sensor registry generated. Groups: {len(alpha_groups)}")
    # Log to conversation_full_history.log
    # Patch: Ensure conversation log directory exists before appending
    conversation_log_path = output_paths.get("conversation_log", "output/conversation_full_history.log")
    conversation_log_path = str(resolve_path(conversation_log_path))
    os.makedirs(os.path.dirname(conversation_log_path), exist_ok=True)
    with open(conversation_log_path, "a") as log:
        log.write("[PATCH-CLUSTERING-REALIGN-V2 EXECUTION] emit_alpha_sensor_registry.py: clustering realigned, outputs updated.\n")
    # --- PATCH-INFER-GAP-TRACE-V1: Entity inclusion/exclusion audit ---
    emit_trace = []
    included_entities = set()
    excluded_entities = set()
    excluded_by_reason = collections.Counter()
    # Build entity_id â†’ fingerprint info (already present as post_entity_info)
    # For every entity in fingerprint_map, log inclusion/exclusion
    for pre_id, info in fingerprint_map.items():
        eid = info.get("post_entity_id")
        area = info.get("area")
        role = info.get("role")
        semantic_role = info.get("semantic_role")
        domain = eid.split(".")[0] if eid else None
        device_class = info.get("device_class")
        name_tokens = eid.split(".")[-1].split("_") if eid else []
        included = False
        reason = None
        # Inclusion logic: must have non-null role and area
        if not eid:
            included = False
            reason = "missing post_entity_id"
        elif not role:
            included = False
            reason = "missing role"
        elif not area:
            included = False
            reason = "missing area"
        else:
            included = True
        emit_trace.append({
            "entity_id": eid,
            "included": included,
            "reason": reason if not included else None,
            "area": area,
            "role": role,
            "semantic_role": semantic_role,
            "domain": domain,
            "device_class": device_class,
            "name_tokens": name_tokens
        })
        if included:
            included_entities.add(eid)
        else:
            excluded_entities.add(eid)
            excluded_by_reason[reason] += 1
    # Emit trace file
    emit_trace_path = str(resolve_path("output/data/alpha_sensor_registry.emit_trace.json"))
    os.makedirs(os.path.dirname(emit_trace_path), exist_ok=True)
    with open(emit_trace_path, 'w') as f:
        json.dump(emit_trace, f, indent=2)
    # Emit summary file
    summary = {
        "total_entities": len(fingerprint_map),
        "included_entities": len(included_entities),
        "excluded_entities": len(excluded_entities),
        "excluded_by_reason": dict(excluded_by_reason)
    }
    summary_path = str(resolve_path("output/data/alpha_sensor_registry.emit_summary.json"))
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2)
    # Console logging: top 5 exclusion reasons and coverage
    print("Top 5 exclusion reasons:")
    for reason, count in excluded_by_reason.most_common(5):
        print(f"  {reason}: {count}")
    coverage = 100.0 * len(included_entities) / max(1, len(fingerprint_map))
    print(f"Entity inclusion coverage: {coverage:.2f}%")
    # Validation: flag if >20% of exclusions have non-null device_class and domain
    flagged = 0
    for rec in emit_trace:
        if not rec["included"] and rec["device_class"] and rec["domain"]:
            flagged += 1
    if len(excluded_entities) > 0 and flagged / len(excluded_entities) > 0.2:
        print(f"WARNING: {flagged} exclusions ({100.0*flagged/len(excluded_entities):.1f}%) have non-null device_class and domain (possible role inference failure)")
    # Remove cluster_area_audit audit log lines (unused and unpopulated)
    # audit_path = str(resolve_config_path("output/data/alpha_sensor_registry.cluster_area_audit.json"))
    # os.makedirs(os.path.dirname(audit_path), exist_ok=True)
    # save_json(cluster_area_audit, audit_path)
    # PATCH-/alpha_sensor-REGISTRY-FREEZE-V1-RECOVERY: Fallback role/semantic_role assignment for clusters
    # For any cluster/entity missing role/semantic_role, apply device_class_role_map and role_inference_rules from config
    for (area, role, semantic_role, tier), group in list(cluster_groups.items()):
        if not role or role == "unknown":
            # Try to infer from any entity in group
            for eid in group["post_reboot_entity_ids"]:
                entity = area_config["entity_map"].get(eid, {})
                device_class = entity.get("device_class")
                if device_class and device_class in settings.get("device_class_role_map", {}):
                    role = settings["device_class_role_map"][device_class]
                    break
            if not role or role == "unknown":
                # Try role_inference_rules
                for eid in group["post_reboot_entity_ids"]:
                    entity = area_config["entity_map"].get(eid, {})
                    for rule in settings.get("role_inference_rules", []):
                        match = rule.get("match", {})
                        if match.get("pattern") and re.search(match["pattern"], eid):
                            role = rule["assign_role"]
                            break
        if not semantic_role or semantic_role == "unknown":
            semantic_role = role
        key = (area, role, semantic_role, tier)
        if semantic_role in feature_roles:
            cluster_groups[key]["features"].extend(group["features"])
        else:
            cluster_groups[key]["post_reboot_entity_ids"].extend(group["post_reboot_entity_ids"])
        cluster_groups[key]["source_clusters"].extend(group["source_clusters"])
        cluster_groups[key]["match_methods"].extend(group["match_methods"])
        cluster_groups[key]["confidence_scores"].extend(group["confidence_scores"])
        cluster_groups[key]["entity_ids"].extend(group["entity_ids"])
    # Final pass: consolidate clusters after role/semantic_role fallback
    alpha_groups_final = []
    debug_log_final = {"clusters": [], "missing_fingerprint": list(missing_fingerprint)}
    seen_entity_ids_final = set()
    seen_incomplete_entity_ids_final = set()
    total_incomplete_clusters_final = 0
    for (area, role, semantic_role, tier), group in list(cluster_groups.items()):
        cluster_id = f"{area}_{role}_{semantic_role}_{tier}"
        post_ids = list(set(group["post_reboot_entity_ids"]))
        features = list(set(group["features"]))
        incomplete = False
        incomplete_reasons = list(group.get("incomplete_reasons", []))
        # PATCH-V3: Incomplete if any reason present or <2 entities
        if incomplete_reasons or not area or not role:
            incomplete = True
        # PATCH-V3: Assert match_methods present
        if not group["match_methods"]:
            print(f"WARNING: cluster {cluster_id} has no match_methods, using fallback from upstream clusters")
        # Warn on duplicates and enforce no entity in both complete and incomplete clusters
        for eid in post_ids:
            if eid in seen_entity_ids_final:
                print(f"WARNING: Duplicate entity_id {eid} in multiple clusters")
            if incomplete:
                seen_incomplete_entity_ids_final.add(eid)
            else:
                seen_entity_ids_final.add(eid)
        if not post_ids:
            continue  # Reject empty clusters
        # Do not allow same entity_id in both complete and incomplete clusters
        if incomplete and any(eid in seen_entity_ids_final for eid in post_ids):
            continue
        if not incomplete and any(eid in seen_incomplete_entity_ids_final for eid in post_ids):
            continue
        cluster_obj = {
            "id": cluster_id,
            "area": area,
            "role": role,
            "semantic_role": semantic_role,
            "tier": tier,
            "confidence_score_mean": sum(group["confidence_scores"]) / max(1, len(group["confidence_scores"])),
            "post_reboot_entity_ids": post_ids,
            "features": features,
            "source_clusters": group["source_clusters"],
            "match_methods": group["match_methods"],
            "incomplete": incomplete
        }
        if incomplete:
            cluster_obj["incomplete_reasons"] = incomplete_reasons
            total_incomplete_clusters_final += 1
        alpha_groups_final.append(cluster_obj)
        debug_log_final["clusters"].append({
            "id": cluster_id,
            "area": area,
            "role": role,
            "semantic_role": semantic_role,
            "tier": tier,
            "post_reboot_entity_ids": post_ids,
            "features": features,
            "incomplete": incomplete,
            "incomplete_reasons": incomplete_reasons,
            "match_methods": group["match_methods"]
        })
    debug_log_final["total_incomplete_clusters"] = [int(total_incomplete_clusters_final)]
    debug_log_final["total_skipped_incomplete_entities"] = [int(len(missing_fingerprint))]
    # PATCH-CLUSTER-GROUPING-BUGFIX-V1: Validate total clusters â‰ˆ unique (area, role) pairs
    unique_area_role_final = set((k[0], k[1]) for k in cluster_groups.keys())
    debug_log_final["unique_area_role_pairs"] = list(unique_area_role_final)
    debug_log_final["total_clusters"] = [len(cluster_groups)]
    debug_log_final["total_unique_area_role_pairs"] = [len(unique_area_role_final)]
    debug_log_final["total_entities_clustered"] = [sum(len(g["post_reboot_entity_ids"]) for g in cluster_groups.values())]
    debug_log_final["skipped_incomplete_entities"] = list(missing_fingerprint)
    # PATCH-AREA-CLUSTER-INTEGRATION-V1: Ensure diagnostics and audit variables are defined for final output
    area_fallback_debug = []
    area_fallback_stats = {}
    area_fallback_missing_device = []
    area_fallback_missing_area = []
    area_fallback_inferred = []
    # --- PATCH: Emit area fallback diagnostics ---
    # Force a test entry to confirm log emission
    area_fallback_debug.append({
        "entity_id": "TEST_ENTITY",
        "device_id": "TEST_DEVICE",
        "fallback_used": True,
        "fallback_area_id": "test_area",
        "fallback_reason": "test_entry"
    })
    area_fallback_stats["forced_test_entry"] = 1
    debug_dir = os.path.join(os.path.dirname(__file__), '../output/debug')
    os.makedirs(debug_dir, exist_ok=True)
    area_fallback_debug_path = os.path.join(debug_dir, 'area_fallback_debug.log')
    area_fallback_summary_path = os.path.join(debug_dir, 'area_fallback_summary.json')
    area_fallback_missing_device_path = os.path.join(debug_dir, 'area_fallback_missing_device.log')
    area_fallback_missing_area_path = os.path.join(debug_dir, 'area_fallback_missing_area.log')
    area_fallback_inferred_path = os.path.join(debug_dir, 'area_fallback_inferred.log')
    with open(area_fallback_debug_path, 'w') as f:
        for entry in area_fallback_debug:
            f.write(json.dumps(entry) + '\n')
    with open(area_fallback_summary_path, 'w') as f:
        json.dump(area_fallback_stats, f, indent=2)
    with open(area_fallback_missing_device_path, 'w') as f:
        for entry in area_fallback_missing_device:
            f.write(json.dumps(entry) + '\n')
    with open(area_fallback_missing_area_path, 'w') as f:
        for entry in area_fallback_missing_area:
            f.write(json.dumps(entry) + '\n')
    with open(area_fallback_inferred_path, 'w') as f:
        for entry in area_fallback_inferred:
            f.write(json.dumps(entry) + '\n')
    # PATCH-AREA-DIAGNOSTIC-MERGE-V2: Normalize entity IDs and log merge failures
    diagnostics_path = str(resolve_path("output/fingerprinting_run/area_resolution_diagnostics.json"))
    merge_failures = []
    def normalize_id(eid):
        if not eid:
            return None
        eid = eid.lower()
        # Remove common suffixes (customize as needed)
        for suffix in ["_alpha_occupancy", "_omega_illumination", "_alpha_motion", "_omega_occupancy", "_alpha_battery", "_alpha_cloud_connection", "_alpha_signal_level", "_alpha_signal_strength", "_alpha", "_omega"]:
            if eid.endswith(suffix):
                eid = eid[: -len(suffix)]
        return eid
    if os.path.exists(diagnostics_path):
        diagnostics = load_json(diagnostics_path)
        diag_map = {}
        for d in diagnostics:
            key = d.get("canonical_key")
            if key:
                diag_map[key] = d
            elif "entity_id" in d:
                diag_map[canonical_entity_key(d["entity_id"])] = d
        merge_successes = []
        merge_failures = []
        merge_trace = []
        inference_traces = []
        for pre_id, info in fingerprint_map.items():
            post_id = info.get("post_entity_id")
            orig_id = info.get("entity_id")
            canon_post_id = canonical_entity_key(post_id)
            canon_orig_id = canonical_entity_key(orig_id)
            # Try diagnostics canonical_key first
            diag, trace = try_canonical_key_match(canon_post_id, diag_map)
            if not diag:
                diag, trace = try_canonical_key_match(canon_orig_id, diag_map)
            inference_traces.append({
                "pre_id": pre_id,
                "post_entity_id": post_id,
                "entity_id": orig_id,
                "canonical_post_entity_id": canon_post_id,
                "canonical_entity_id": canon_orig_id,
                "trace": trace
            })
            if diag:
                info["final_area"] = diag.get("final_area", info.get("area"))
                info["area_inference_source"] = diag.get("area_inference_source", info.get("area_inference_source", "unknown"))
                # PATCH: Also set info["area"] for downstream compatibility
                info["area"] = info["final_area"]
                merge_successes.append({
                    "pre_id": pre_id,
                    "post_entity_id": post_id,
                    "entity_id": orig_id,
                    "canonical_post_entity_id": canon_post_id,
                    "canonical_entity_id": canon_orig_id,
                    "final_area": info["final_area"],
                    "area_inference_source": info["area_inference_source"],
                    "match_type": trace["result"].get("match_type") if trace and trace.get("result") else None
                })
                merge_trace.append({
                    "pre_id": pre_id,
                    "post_entity_id": post_id,
                    "entity_id": orig_id,
                    "canonical_post_entity_id": canon_post_id,
                    "canonical_entity_id": canon_orig_id,
                    "final_area": info["final_area"],
                    "area_inference_source": info["area_inference_source"],
                    "match_type": trace["result"].get("match_type") if trace and trace.get("result") else None,
                    "result": "success"
                })
            else:
                merge_failures.append({
                    "pre_id": pre_id,
                    "post_entity_id": post_id,
                    "entity_id": orig_id,
                    "canonical_post_entity_id": canon_post_id,
                    "canonical_entity_id": canon_orig_id,
                    "reason": "no match in diagnostics"
                })
                merge_trace.append({
                    "pre_id": pre_id,
                    "post_entity_id": post_id,
                    "entity_id": orig_id,
                    "canonical_post_entity_id": canon_post_id,
                    "canonical_entity_id": canon_orig_id,
                    "result": "failure"
                })
        # Emit logs to /output/data/ and PATCH-INFERRED-AREA-CLUSTER-FINAL-V2.log
        data_dir = os.path.join(os.path.dirname(__file__), '../output/data')
        os.makedirs(data_dir, exist_ok=True)
        with open(os.path.join(data_dir, 'area_merge_successes.json'), 'w') as f:
            json.dump(merge_successes, f, indent=2)
        with open(os.path.join(data_dir, 'area_merge_failures_detailed.json'), 'w') as f:
            json.dump(merge_failures, f, indent=2)
        with open(os.path.join(data_dir, 'area_merge_trace.log'), 'w') as f:
            for entry in merge_trace:
                f.write(json.dumps(entry) + '\n')
        patch_log_path = os.path.join(os.path.dirname(__file__), '../copilot_patches/PATCH-INFERRED-AREA-CLUSTER-FINAL-V2.log')
        with open(patch_log_path, 'a') as log:
            log.write('[PATCH-INFERRED-AREA-CLUSTER-FINAL-V2] Canonical key merge, area-driven clustering, and diagnostics complete.\n')
    # PATCH-INFERRED-AREA-CLUSTER-FINAL-V3: Add fuzzy matching (Levenshtein â‰¤2), variant table, prefix token matching, and log all canonical key inference attempts/results to canonical_key_inference_trace.json. Use these strategies in order for diagnostics merge. Ensure all patch logs are updated.
    if os.path.exists(diagnostics_path):
        diagnostics = load_json(diagnostics_path)
        diag_map = {canonical_entity_key(d["entity_id"]): d for d in diagnostics if "entity_id" in d}
        merge_successes = []
        merge_failures = []
        merge_trace = []
        inference_traces = []
        for pre_id, info in fingerprint_map.items():
            post_id = info.get("post_entity_id")
            orig_id = info.get("entity_id")
            canon_post_id = canonical_entity_key(post_id)
            canon_orig_id = canonical_entity_key(orig_id)
            # Try diagnostics canonical_key first
            diag, trace = try_canonical_key_match(canon_post_id, diag_map)
            if not diag:
                diag, trace = try_canonical_key_match(canon_orig_id, diag_map)
            inference_traces.append({
                "pre_id": pre_id,
                "post_entity_id": post_id,
                "entity_id": orig_id,
                "canonical_post_entity_id": canon_post_id,
                "canonical_entity_id": canon_orig_id,
                "trace": trace
            })
            if diag:
                info["final_area"] = diag.get("final_area", info.get("area"))
                info["area_inference_source"] = diag.get("area_inference_source", info.get("area_inference_source", "unknown"))
                # PATCH: Also set info["area"] for downstream compatibility
                info["area"] = info["final_area"]
                merge_successes.append({
                    "pre_id": pre_id,
                    "post_entity_id": post_id,
                    "entity_id": orig_id,
                    "canonical_post_entity_id": canon_post_id,
                    "canonical_entity_id": canon_orig_id,
                    "final_area": info["final_area"],
                    "area_inference_source": info["area_inference_source"],
                    "match_type": trace["result"].get("match_type") if trace and trace.get("result") else None
                })
                merge_trace.append({
                    "pre_id": pre_id,
                    "post_entity_id": post_id,
                    "entity_id": orig_id,
                    "canonical_post_entity_id": canon_post_id,
                    "canonical_entity_id": canon_orig_id,
                    "final_area": info["final_area"],
                    "area_inference_source": info["area_inference_source"],
                    "match_type": trace["result"].get("match_type") if trace and trace.get("result") else None,
                    "result": "success"
                })
            else:
                merge_failures.append({
                    "pre_id": pre_id,
                    "post_entity_id": post_id,
                    "entity_id": orig_id,
                    "canonical_post_entity_id": canon_post_id,
                    "canonical_entity_id": canon_orig_id,
                    "reason": "no match in diagnostics"
                })
                merge_trace.append({
                    "pre_id": pre_id,
                    "post_entity_id": post_id,
                    "entity_id": orig_id,
                    "canonical_post_entity_id": canon_post_id,
                    "canonical_entity_id": canon_orig_id,
                    "result": "failure"
                })
        # Emit logs to /output/data/ and PATCH-INFERRED-AREA-CLUSTER-FINAL-V3.log
        data_dir = os.path.join(os.path.dirname(__file__), '../output/data')
        os.makedirs(data_dir, exist_ok=True)
        with open(os.path.join(data_dir, 'area_merge_successes.json'), 'w') as f:
            json.dump(merge_successes, f, indent=2)
        with open(os.path.join(data_dir, 'area_merge_failures_detailed.json'), 'w') as f:
            json.dump(merge_failures, f, indent=2)
        with open(os.path.join(data_dir, 'area_merge_trace.log'), 'w') as f:
            for entry in merge_trace:
                f.write(json.dumps(entry) + '\n')
        with open(os.path.join(os.path.dirname(__file__), '../output/canonical_key_inference_trace.json'), 'w') as f:
            json.dump(inference_traces, f, indent=2)
        patch_log_path = os.path.join(os.path.dirname(__file__), '../copilot_patches/PATCH-INFERRED-AREA-CLUSTER-FINAL-V3.log')
        with open(patch_log_path, 'a') as log:
            log.write('[PATCH-INFERRED-AREA-CLUSTER-FINAL-V3] Fuzzy, variant, and prefix matching for canonical key merge, full inference trace emitted.\n')

if __name__ == "__main__":
    main()
